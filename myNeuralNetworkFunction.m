function [y1] = myNeuralNetworkFunction()
%MYNEURALNETWORKFUNCTION neural network simulation function.
%
% Generated by Neural Network Toolbox function genFunction, 16-May-2016 19:20:47.
%
% [y1] = myNeuralNetworkFunction() takes these arguments:
% and returns:
%   y = Qx2 matrix, output #1
% where Q is the number of samples.

%#ok<*RPMT0>

% ===== NEURAL NETWORK CONSTANTS =====

% Layer 1
b1 = [0.042556477663271669032;0.099337891098396080558;-0.2393310308594215341;-0.10498350388404106837;-0.11716169475603188666;0.18460345830636973319;0.037648202000411710999;-0.013546975023314967307;-0.109229980373639457;-0.050936164482698803591];

% Layer 2
b2 = [0.45671109166739176644;0.08624079394923267472];
LW2_1 = [-0.7022465593249018756 -0.099212838695768912345 0.79930198101816163359 0.76497261480059597361 0.34645197290451545946 -0.75437001220094823672 -0.44942609736861993674 -0.4332312367421273791 0.65315778462712037467 -0.0041941140716268421329;0.79942696907147703378 -0.58865532107207307888 0.52517107874645385657 -0.43009956395035753296 0.32855980883610147458 -0.1853631538863742545 0.43333948127828336716 0.79239771369899081321 -0.2199469743720046111 0.38961038534498770858];

% Output 1
y1_step1_ymin = -1;
y1_step1_gain = [0.00632911392405063;0.00628930817610063];
y1_step1_xoffset = [105;31];

% ===== SIMULATION ========

% Dimensions
Q = 0; % samples

% Layer 1
a1 = tansig_apply(repmat(b1,1,Q));

% Layer 2
a2 = repmat(b2,1,Q) + LW2_1*a1;

% Output 1
y1 = mapminmax_reverse(a2,y1_step1_gain,y1_step1_xoffset,y1_step1_ymin);
y1 = y1';
end

% ===== MODULE FUNCTIONS ========

% Sigmoid Symmetric Transfer Function
function a = tansig_apply(n)
a = 2 ./ (1 + exp(-2*n)) - 1;
end

% Map Minimum and Maximum Output Reverse-Processing Function
function x = mapminmax_reverse(y,settings_gain,settings_xoffset,settings_ymin)
x = bsxfun(@minus,y,settings_ymin);
x = bsxfun(@rdivide,x,settings_gain);
x = bsxfun(@plus,x,settings_xoffset);
end
